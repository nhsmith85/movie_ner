{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Format Conversion - spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOWiS1KR/sa6sKxnK8nMCh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyvi8mYFwLcO"
      },
      "source": [
        "# spaCy Formatting\n",
        "This notebook covers the process for preparing data to run through a spaCy NER model. The code can be reused for any dataset where docs are tokenized in lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsCQqwmLvQfK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc50fd48-39f3-4164-b741-2b9578db46ed"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH6lbE65vZVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70b36bb7-3195-4dea-a95d-e3fa4aecbb3e"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/DAAN888/data')\n",
        "#os.chdir('/content/drive/My Drive/DAAN888/data')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/DAAN888/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62cOVjV3NYk6"
      },
      "source": [
        "### Function for Converting to spaCy format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKqCu84NNWMZ"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.gold import biluo_tags_from_offsets, iob_to_biluo, offsets_from_biluo_tags\n",
        "\n",
        "nlp = spacy.load('en', vectors=False, disable=['tagger', 'parser', 'ner'])\n",
        "nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
        "\n",
        "\n",
        "def spacy_formatting(docs, labels):\n",
        "  ''' docs and labels must be in list of lists format where docs are \n",
        "  already tokenized '''\n",
        "\n",
        "  result_docs, result_ents = [], []\n",
        "  for doc, tags in list(zip(docs, labels)):\n",
        "\n",
        "    biluos = iob_to_biluo(tags) # convert to bilou format\n",
        "\n",
        "    DOC = Doc(nlp.vocab, words=doc) # make the doc object that's required\n",
        "\n",
        "    ents = {}\n",
        "    ents['entities'] = offsets_from_biluo_tags(DOC, biluos)\n",
        "\n",
        "    doc = ' '.join(doc)\n",
        "\n",
        "    result_docs.append(doc)\n",
        "    result_ents.append(ents)\n",
        "    \n",
        "  return list(zip(result_docs, result_ents)) # return original tokens and their offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knU0xtO1N7RO"
      },
      "source": [
        "### MIT MOVIE CONVERSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Emw_5U3vZaq"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('mitmovie.pickle', mode = 'rb') as handle:\n",
        "  dataset = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hl5rAbSvZcv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b1e1656-68a2-4957-f91a-67f1a3d27be4"
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train_tokens', 'train_labels', 'test_tokens', 'test_labels'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcofr-4j5_Yp"
      },
      "source": [
        "train = spacy_formatting(dataset['train_tokens'], dataset['train_labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtDwxYj8Pdbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f18c16c7-c43a-4226-dde4-14fb57bde093"
      },
      "source": [
        "# check a result\n",
        "train[0], dataset['train_tokens'][0], dataset['train_labels'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('what movies star bruce willis', {'entities': [(17, 29, 'ACTOR')]}),\n",
              " ['what', 'movies', 'star', 'bruce', 'willis'],\n",
              " ['O', 'O', 'O', 'B-ACTOR', 'I-ACTOR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFYmxsxSPO_P"
      },
      "source": [
        "test = spacy_formatting(dataset['test_tokens'], dataset['test_labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Is92ztxP1yd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9e785b85-b136-4c02-93f7-256fae13eaed"
      },
      "source": [
        "# check a result\n",
        "test[0], dataset['test_tokens'][0], dataset['test_labels'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('are there any good romantic comedies out right now',\n",
              "  {'entities': [(19, 36, 'GENRE'), (41, 50, 'YEAR')]}),\n",
              " ['are',\n",
              "  'there',\n",
              "  'any',\n",
              "  'good',\n",
              "  'romantic',\n",
              "  'comedies',\n",
              "  'out',\n",
              "  'right',\n",
              "  'now'],\n",
              " ['O', 'O', 'O', 'O', 'B-GENRE', 'I-GENRE', 'O', 'B-YEAR', 'I-YEAR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmN89eEQerr"
      },
      "source": [
        "import json\n",
        "\n",
        "# save as json\n",
        "with open('spacy_mitmovie_train.json', 'w') as handle:\n",
        "  json.dump(train, handle)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CAMvegMRAdq"
      },
      "source": [
        "with open('spacy_mitmovie_test.json', 'w') as handle:\n",
        "  json.dump(test, handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YICqtrBWPZlL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ujm8gOERM7s"
      },
      "source": [
        "### GMB DATASET CONVERSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9U8yEbURPLJ"
      },
      "source": [
        "with open('gmb.pickle', mode = 'rb') as handle:\n",
        "  dataset = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkY4YbIgRVuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19549bc2-ca4c-47d4-d54f-37f7d2075184"
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tokens', 'labels'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QX2Qv7CRYs8"
      },
      "source": [
        "gmb = spacy_formatting(dataset['tokens'], dataset['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFJ0cZiPSOSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "e0c0913d-2551-406e-a920-db83633b310c"
      },
      "source": [
        "gmb[0], list(zip(dataset['tokens'][0], dataset['labels'][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .',\n",
              "  {'entities': [(48, 54, 'geo'), (77, 81, 'geo'), (111, 118, 'gpe')]}),\n",
              " [('Thousands', 'O'),\n",
              "  ('of', 'O'),\n",
              "  ('demonstrators', 'O'),\n",
              "  ('have', 'O'),\n",
              "  ('marched', 'O'),\n",
              "  ('through', 'O'),\n",
              "  ('London', 'B-geo'),\n",
              "  ('to', 'O'),\n",
              "  ('protest', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('war', 'O'),\n",
              "  ('in', 'O'),\n",
              "  ('Iraq', 'B-geo'),\n",
              "  ('and', 'O'),\n",
              "  ('demand', 'O'),\n",
              "  ('the', 'O'),\n",
              "  ('withdrawal', 'O'),\n",
              "  ('of', 'O'),\n",
              "  ('British', 'B-gpe'),\n",
              "  ('troops', 'O'),\n",
              "  ('from', 'O'),\n",
              "  ('that', 'O'),\n",
              "  ('country', 'O'),\n",
              "  ('.', 'O')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liW4HwgWS3Sv"
      },
      "source": [
        "# save as json\n",
        "\n",
        "with open('spacy_gmb.json', 'w') as handle:\n",
        "  json.dump(gmb, handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVxrHdATEth"
      },
      "source": [
        "## Unused Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDjDZ82_-zYq"
      },
      "source": [
        "def convert_to_spacy(tokens, tags):\n",
        "  ''' will convert list of docs into spacy span format\n",
        "  edited but referenced from: https://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba \n",
        "  '''\n",
        "\n",
        "  word_tag = []\n",
        "  for words, labels in zip(tokens, tags ):\n",
        "    word_tag.append(list(zip(words, labels)))\n",
        "\n",
        "  \n",
        "  spacy_format, entities, sentence, unique_labels = [], [], [], []\n",
        "  current_annotation = None\n",
        "  end = 0 # initialize counter to keep track of start and end characters\n",
        "  for doc in word_tag:\n",
        "    for i, (w, l) in enumerate(doc):\n",
        "      label = l[2:]\n",
        "      label_type = l[0]\n",
        "      sentence.append(w)\n",
        "      end += (len(w) + 1)\n",
        "\n",
        "      if label_type != 'I' and current_annotation: # if at end of annotation\n",
        "        entities.append((start, end - 2 - len(w), current_annotation)) # append the annotation\n",
        "        current_annotation = None # reset\n",
        "      if label_type == 'B': # if beginning of an annotation\n",
        "        start = end - len(w) - 1\n",
        "        current_annotation = label\n",
        "      if label_type == 'I': # if annotation is multi-word\n",
        "        current_annotation = label\n",
        "      if label != 'O' and label not in unique_labels:\n",
        "        unique_labels.append(label)\n",
        "\n",
        "      if i == (len(doc) -  1):\n",
        "        if current_annotation: # if there was an annotation\n",
        "          entities.append((start, end - 1, current_annotation))\n",
        "        sentence = ' '.join([w for w,l in doc])\n",
        "        spacy_format.append([sentence, {'entities' : entities}])\n",
        "        # reset the counters and temporary lists\n",
        "        end = 0            \n",
        "        entities, sentence = [], []\n",
        "        current_annotation = None\n",
        "  \n",
        "  return spacy_format, unique_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dthdNLnnwq1n"
      },
      "source": [
        "#!head engtrain.bio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Op34U_1xY9O"
      },
      "source": [
        "# used for extracting from bio format\n",
        "# https://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba \n",
        "\n",
        "# def load_data_spacy(file_path):\n",
        "#     ''' Converts data from:\n",
        "#     label \\t word \\n label \\t word \\n \\n label \\t word\n",
        "#     to: sentence, {entities : [(start, end, label), (stard, end, label)]}\n",
        "#     '''\n",
        "#     file = open(file_path, 'r')\n",
        "#     training_data, entities, sentence, unique_labels = [], [], [], []\n",
        "#     current_annotation = None\n",
        "#     end = 0 # initialize counter to keep track of start and end characters\n",
        "#     for line in file:\n",
        "#         line = line.strip(\"\\n\").split(\"\\t\")\n",
        "#         # lines with len > 1 are words\n",
        "#         if len(line) > 1:\n",
        "#             label = line[0][2:]     # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
        "#             label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
        "#             word = line[1]\n",
        "#             sentence.append(word)\n",
        "#             end += (len(word) + 1)  # length of the word + trailing space\n",
        "           \n",
        "#             if label_type != 'I' and current_annotation:  # if at the end of an annotation\n",
        "#                 entities.append((start, end - 2 - len(word), current_annotation))  # append the annotation\n",
        "#                 current_annotation = None                 # reset the annotation\n",
        "#             if label_type == 'B':                         # if beginning new annotation\n",
        "#                 start = end - len(word) - 1  # start annotation at beginning of word\n",
        "#                 current_annotation = label   # append the word to the current annotation\n",
        "#             if label_type == 'I':            # if the annotation is multi-word\n",
        "#                 current_annotation = label   # append the word\n",
        "           \n",
        "#             if label != 'O' and label not in unique_labels:\n",
        "#                 unique_labels.append(label)\n",
        " \n",
        "#         # lines with len == 1 are breaks between sentences\n",
        "#         if len(line) == 1:\n",
        "#             if current_annotation:\n",
        "#                 entities.append((start, end - 1, current_annotation))\n",
        "#             sentence = \" \".join(sentence)\n",
        "#             training_data.append([sentence, {'entities' : entities}])\n",
        "#             # reset the counters and temporary lists\n",
        "#             end = 0            \n",
        "#             entities, sentence = [], []\n",
        "#             current_annotation = None\n",
        "#     file.close()\n",
        "#     return training_data, unique_labels            \n",
        "           \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14n_r5OFxY_h"
      },
      "source": [
        "#TRAIN_DATA, LABELS = load_data_spacy(\"engtrain.bio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJdr0lL08AFo"
      },
      "source": [
        "#TRAIN_DATA[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMPOGA3KydSN"
      },
      "source": [
        "# python -m spacy convert [input_file] [output_dir] [--file-type] [--converter]\n",
        "# [--n-sents] [--morphology] [--lang]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mH1xjHKx9mt"
      },
      "source": [
        "#!python -m spacy convert engtrain.bio -t json -c ner\n",
        "# not working"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfrZ-DqyxYnO"
      },
      "source": [
        "# We should explore the pretraining option, which trains on the corpus of text you have and learns word embeddings https://spacy.io/usage/vectors-similarity\n",
        "# it does not require annotations - and it might help the performance of our models\n",
        "# but for now we will just get the data formatted for entry into a spacy model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaH12Ohpy4LH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}